<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="WavBench: Benchmarking both IQ and EQ of End-to-End Spoken Dialogue Models.">
  <meta name="keywords" content="WavBench, Spoken Dialogue Models, IQ, EQ, Benchmark, Paralinguistics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>WavBench: Benchmarking IQ and EQ of End-to-End Spoken Dialogue Models</title>
  <style>
    body { font-family: sans-serif; margin: 20px; line-height: 1.6; }
    header, section, footer { margin-bottom: 30px; }
    h1, h2, h3 { margin-bottom: 10px; }
    .publication-title { font-size: 2em; text-align: center; }
    .authors, .affiliation { text-align: center; margin-bottom: 10px; }
    .publication-links { text-align: center; margin-bottom: 20px; }
    .publication-links span { margin-right: 10px; }
    .content { text-align: justify; }
    pre { background-color: #f4f4f4; padding: 10px; overflow-x: auto; }
    ul { margin-left: 20px; }
    a { color: #007bff; text-decoration: none; }
    a:hover { text-decoration: underline; }
    .text-center { text-align: center; }
  </style>
</head>
<body>

<header>
  <h1 class="publication-title">WavBench: Benchmarking both IQ and EQ of End-to-End Spoken Dialogue Models</h1>
  <div class="authors">
    <p>Anonymous Author(s)</p>
  </div>
  <div class="affiliation">
    <p>Affiliation<br>Address<br>email</p>
  </div>
  <div class="publication-links">
    <span><a href="#">Paper </a></span>
    <span><a href="#">arXiv (coming soon)</a></span>

    <span><a href="https://github.com/wavbench/WavBench">Tmp Code (Anonymous GitHub)</a></span>
    <span><a href="https://huggingface.co/datasets/sgshdgdhsdg/WavDataset">Tmp Dataset (Anonymous HuggingFace)</a></span>
  </div>
</header>

<section id="teaser-summary">
  <h2 class="text-center">Overview</h2>
  <p class="content">
    WavBench is the first benchmark capable of evaluating both the Intelligent Quotient (IQ) and Emotional Quotient (EQ) of end-to-end spoken dialogue models. It offers a comprehensive framework for assessing paralinguistic understanding and generation in conversational AI.
  </p>
</section>

<section id="abstract">
  <h2>Abstract</h2>
  <div class="content">
    <p>
      Recently, spoken dialogue models have transitioned from text-centric ASR, LLM,
      and TTS cascaded dialogue systems to speech-centric end-to-end dialogue systems.
      These end-to-end spoken dialogue models not only retain the intelligent quotient
      (IQ) of large language models but also emphasize the emotional quotient (EQ)
      related to paralinguistic information. They are capable of role-playing, style con-
      trol, and responding with the most appropriate tone for different scenarios.
      However, current benchmarking frameworks for spoken dialogue models primar-
      ily focus on text-based dialogue content evaluation and acoustic information
      understanding ("what to say"), with little attention paid to directly evaluating
      the speech output of end-to-end spoken dialogue models. This paper introduces
      WavBench, the first benchmark capable of evaluating both the IQ and EQ of end-
      to-end spoken dialogue models. Specifically: 1) the capability to assess both
      content and different acoustic features, including age, accent, language, gender,
      emotion, pitch, speed, volume, audio, and music; 2) the capability to evaluate the
      quality of model speech responses in both explicit and implicit dialogue settings
      ("both what to say and how to say"). Through comprehensive evaluation of ten
      end-to-end spoken dialogue models, WavBench demonstrates their strengths and
      limitations in paralinguistic understanding and generation, offering insights into the
      evolving landscape of emotionally intelligent spoken AI. The benchmark dataset
      and evaluation toolkit are available at https://wavbench.github.io/.
    </p>
  </div>
</section>

<section id="examples-note">
    <h2>Illustrative Examples</h2>
    <p class="content">
      The WavBench dataset includes a variety of explicit and implicit dialogue scenarios designed to test different paralinguistic features.
      For detailed examples showcasing specific tasks such as emotion perception, accent generation, and multi-turn implicit chat evaluations, please refer to Figure below.
    </p>
</section>

<div style="text-align: center;">
        <img src="wavbench.png" alt="Word Clouds Example">
    </div>
<!-- 

<p align="center">The figure below shows two example word clouds for style descriptions.</p>
<br>
<img src="figure2.png">
<br> -->

<section id="key-features">
  <h2>Key Features of WavBench</h2>
  <div class="content">
    <p>
      WavBench introduces a novel approach to benchmarking end-to-end spoken dialogue systems by focusing on both their understanding and generation capabilities, particularly concerning paralinguistic information.
    </p>
    <ul>
      <li><strong>Comprehensive Evaluation:</strong> Assesses both Intelligent Quotient (IQ - "what to say") and Emotional Quotient (EQ - "how to say").</li>
      <li><strong>10 Paralinguistic Dimensions:</strong> Evaluates speaker information (age, gender, accent, language), acoustic characteristics (pitch, speed, volume, emotion), and background sounds (audio, music).</li>
      <li><strong>Explicit and Implicit Settings:</strong> Tests models on their ability to follow direct instructions regarding paralinguistics (explicit) and to infer and respond appropriately in natural conversation (implicit).</li>
      <li><strong>Understanding and Generation:</strong> Uniquely focuses on evaluating the paralinguistic features in the model's generated speech output, not just its understanding of input speech.</li>
      <li><strong>Real-world Relevance:</strong> Dialogue scenarios are designed to reflect realistic human interactions, pushing for more natural and emotionally intelligent AI.</li>
      <li><strong>Open Resource:</strong> The WavBench dataset and evaluation toolkit are publicly available to facilitate further research and development.</li>
    </ul>
    <p>
      Figure 1 in the paper illustrates the emotional quotient gap WavBench aims to address. For specific task examples, please refer to Figure 2 in the paper.
    </p>
  </div>
</section>



<section id="related-future">
  <h2>Related Links & Future Work</h2>
  <div class="content">
    <p>
      WavBench builds upon and complements existing benchmarks in spoken language understanding and dialogue systems. For a detailed comparison, please refer to Table 1 in our paper.
    </p>
    
  </div>
</section>

<!-- <section id="bibtex">
  <h2>BibTeX</h2>
  <pre><code>@misc{wavbench2025anonymous,
  author    = {Anonymous Author(s)},
  title     = {WavBench: Benchmarking both {IQ} and {EQ} of End-to-End Spoken Dialogue Models},
  howpublished = {Submitted to 39th Conference on Neural Information Processing Systems (NeurIPS 2025)},
  year      = {2024},
  note      = {Dataset and toolkit available at \url{https://wavbench.github.io/}}
}</code></pre>
</section> -->

<!-- <footer>
  <p class="text-center">
    For more information, visit the <a href="https://wavbench.github.io/">WavBench project page</a>.
  </p>
  <p class="text-center" style="font-size: 0.8em; color: #777;">
    Page structure based on a standard academic project page.
  </p>
</footer> -->

</body>
</html>
